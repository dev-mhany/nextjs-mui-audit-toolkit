Statement of Work: Next.js + MUI Code Audit Platform (GitHub Integration)
Project Overview and Objectives
We propose to develop an open-source Next.js + MUI Code Auditing Platform – a web application and accompanying GitHub integration that will be the "GOAT" (Greatest of All Time) solution for auditing code directly from GitHub repositories. This platform will allow both an internal team and any external user to easily audit their Next.js projects (with Material-UI) for best practices, performance, accessibility, and code quality. The solution will leverage a powerful auditing toolkit (Next.js + MUI Best-Practice Audit Toolkit v1.1.0) and integrate tightly with GitHub via a GitHub App for a seamless, secure workflow. The end goals and objectives include:
• Automated Best-Practice Auditing: Provide a one-click or one-command audit of a given GitHub repository's code, checking Next.js architecture, MUI usage, accessibility (WCAG), responsive design, performance, security, code quality, and testing/documentation standards.
• Integration with GitHub Workflow: Ensure results are delivered through GitHub (pull requests, check runs, comments) so developers can view and act on findings in their normal workflow.
• Accessible to Any User: The platform should support use by the internal team and external users. It will be open-sourced and easily deployable (the client will host an instance on Vercel for general use).
• Security and Privacy: Use GitHub's OAuth App installation (preferred) or personal access tokens (fallback) to access repositories with least privileges, ensuring no long-term secret storage. All audits run in sandboxed GitHub Action runners (not on our server) for safety.
• User-Friendly UI/UX: Provide a simple web front-end where users can input a repo URL, configure options via checkboxes (e.g. static vs. full audit, create PR vs. just branch, auto-fix enabled, etc.), and see live status and results. No deep technical knowledge needed to initiate an audit.
• Comprehensive Reporting: Deliver detailed audit results including an overall grade (A–F), category scores, and line-level issue findings. Provide multiple report formats (interactive HTML dashboard, Markdown report, JSON data) and automatic suggestions/fixes for certain issues – making it easier to improve the code base.
• Open Source & Extensible: Release the platform under an open-source license, with clear documentation, so the community can use it, contribute to it, or extend it (e.g. plugin system for custom rules).
By meeting these objectives, this project will produce a state-of-the-art code auditing service that elevates code quality for Next.js and MUI projects, with minimal friction for developers.
Scope of Work and Deliverables
The scope covers all components needed to implement the complete solution: GitHub integration (App and workflows), backend services, front-end application, integration of the audit toolkit, result reporting, and deployment. Below is a breakdown of key deliverables and features:

1. GitHub App Integration and Access Control:
2. GitHub App Setup: Create a GitHub App (e.g. “Nextjs-MUI Auditor”) with fine-grained permissions limited to a single repository install. Required scopes include:
   o Contents: read & write (to read code and commit the audit results branch).
   o Pull Requests: write (to open/update a PR with the audit results).
   o Checks: write (to post status checks summarizing the audit).
   o Metadata: read (basic repo info).
   (Note: If needed, Actions: read permission for triggering workflows, though repository dispatch events may use the token directly.)
3. OAuth Installation Flow: Implement an OAuth flow for users to install the GitHub App on their repository. The front-end will include an “Install GitHub App” button that directs the user to GitHub to install the app on the desired repo or organization. After installation, handle the callback (/auth/github/callback) to capture the installation id. Ensure the process works for organizations with SSO or branch protection requirements.
4. Personal Access Token (PAT) Fallback: Provide a fallback for users who cannot or prefer not to use the GitHub App. The UI will allow entering a GitHub Personal Access Token (with minimum scopes like repo and possibly workflow). Clear warnings will be shown about security, urging use of the App. The backend will not store the PAT; it will only be used transiently for the audit trigger.
5. Access Verification: On audit initiation, verify we have the needed access to the target repo (via installation token or PAT). If the GitHub App is installed, generate a short-lived installation token on-demand (using GitHub App credentials) – no long-term secrets stored. If using PAT, validate its scopes.
6. Automated Audit Workflow (GitHub Actions):
   We will implement an automated CI workflow to perform the actual audit on the target repository via GitHub Actions, ensuring that all code execution is sandboxed on GitHub runners. Two workflows are involved: a trigger workflow in our platform’s repo, and a receiver workflow in the target repo. Deliverables include:
7. Reusable Trigger Workflow: In the platform’s repository, create a reusable GitHub Actions workflow (e.g. .github/workflows/audit.yml) that can dispatch an audit to any repository. This workflow will:
   o Use the GitHub App’s credentials (via the stored app ID and private key) to generate an installation token for the target repo (using an action like actions/create-github-app-token@v1).
   o Call the GitHub API to trigger a repository dispatch event on the target repository (using an action such as peter-evans/repository-dispatch@v3). We will send an event type (e.g. dev-mhany-audit or similar) with a payload specifying the target git reference (branch or commit to audit, defaulting to default branch) and any options (like whether to include runtime tests).
   o This workflow allows our backend to invoke it easily by calling GitHub’s API, keeping our server logic minimal. It will run on-demand when the backend triggers it via the GitHub API (with repo permissions or using the app token).
8. Receiver Audit Workflow (Target Repo): Provide a workflow file to be added to users’ repositories (e.g. .github/workflows/run-audit.yml). This is the workflow that runs in the context of the target repository when triggered:
   o Checkout Code: It will run on GitHub-hosted runners (Ubuntu) and check out the repository code (optionally at the specific ref if provided in payload). Use a shallow checkout or depth=1 for performance unless a full history is needed.
   o Setup Environment: Install Node.js (v18+) and any needed tools. For example, use actions/setup-node@v4 to set up Node 18.
   o Install Audit Toolkit: Globally install or cache the Next.js+MUI Audit Toolkit (e.g. npm i -g nextjs-mui-audit-toolkit).
   o Run Audit Command: Execute the audit toolkit CLI, e.g. npx nextjs-mui-audit run --output ./audit --min-score 85 --strict. This performs the comprehensive static analysis (and optionally runtime tests if enabled, see next point). It will generate audit result files in an audit/ directory. By default, it will run static analysis; if the user requested full audits (including runtime tests like Lighthouse, E2E), the workflow will handle that (possibly via an input flag or by detecting a label in the dispatch payload). For security, runtime tests that execute the app (Lighthouse, Playwright) can be gated behind an explicit user opt-in.
   o The audit includes numerous checks (outlined in section 5) and produces an overall score and pass/fail status (with configurable minimum score or critical issue thresholds). The --strict flag ensures any critical issues or score below threshold will cause a non-zero exit.
   o Commit Audit Results to New Branch: After audit completes, the workflow will create a new branch on the repository, e.g. chore/audit-<YYYYMMDD-HHMMSS>, and commit the results. Deliverables from each run include:
   o audit/REPORT.md – a detailed Markdown report (human-readable summary of findings).
   o audit/report.html – an interactive HTML report (with charts, filtering, etc.).
   o audit/report.json – machine-readable JSON with all findings and scores.
   o audit/FIX_REPORT.md – a summary of any fixes applied (if auto-fix was enabled).
   These files will be committed to the new branch (with commit message prefix “audited by [bot]” for clarity). No files are written to the default branch to avoid clutter or bypassing protections.
   o Open or Update Pull Request: The workflow will then create a Pull Request (PR) from the audit branch into the default branch (if a PR doesn’t already exist), or update an existing open audit PR for that branch. We will use an action like peter-evans/create-pull-request@v6 for this. The PR will have a title like “Audit: Next.js + MUI (Automated)” and a body containing a brief summary: e.g. “Automated audit by dev-mhany. - Overall grade: see audit/REPORT.md in this PR - HTML report and JSON are attached in the audit/ folder.” It can also include quick highlights (e.g. number of critical issues). The PR will be labeled (e.g. audit and automated) for easy filtering.
   o Post Status Check Summary: Finally, the workflow will use the GitHub Checks API to post a status check on the commit/PR with a high-level summary (using an action like LouisBrunner/checks-action). For example, it will create a check named “Audit Summary” with a success/failure status and a short summary (e.g. “Audit complete – overall grade B (87). See audit/report.html for full details.”). This surfaces the result prominently in the GitHub UI (Checks tab and PR status).
   o Optional Auto-Merge: (If requested/allowed) The system can support automatically merging the audit PR if all checks pass and the grade meets the minimum. This could be enabled via a special label (e.g. audit:auto-merge) or user option. If enabled and the audit result is satisfactory (no failing checks), the workflow or a bot could merge the PR. This behavior will be carefully controlled to respect branch protections and only occur when explicitly authorized.
9. Result Artifacts & Cleanup: The audit results live in the PR as files. We will not use external artifact storage by default (simpler access via PR). Users can download the reports from the PR, or view them directly (the Markdown report can be read in PR, the HTML report can be served via raw file view or a Pages branch if desired). The workflow will be designed to keep artifact sizes reasonable (reports are mostly text/HTML). Caching can be used for subsequent runs (see point 5). We will also ensure that repeated audits don’t flood the repo with branches – e.g., possibly close older audit PRs or reuse a single audit branch per repository (though a timestamp in branch allows multiple runs; we can clean up old branches periodically or label them).
10. Frontend Web Application (UI/UX) – Vercel Hosted:
    Develop a Next.js front-end (likely using React + Material-UI for consistency) to serve as the user interface for the audit platform. This front-end will be deployed on Vercel (for global availability and easy integration). Key features and deliverables for the UI:
11. Repository Input & Validation: A home page with a clear input form for the GitHub repository to audit. The user can enter a repo URL (https://github.com/owner/repo) or owner/repo name. The UI will validate the format and possibly check if our App is installed for that repo (if the user is authenticated).
12. Authentication & App Installation Prompt: If the user is not yet authenticated or hasn’t installed the GitHub App, the UI will guide them. Provide a “Connect with GitHub” or “Install App” button. Once clicked, it initiates the GitHub App OAuth flow. After returning from installation, the UI can show which repositories the app is installed on (via GitHub API). If the entered repo is not yet authorized, prompt the user to install the app on that repo (or use PAT as alternative).
13. Configuration Options via Checkboxes/Toggles: The UI will offer several checkboxes or toggle switches so the user can customize the audit run:
    o “Full Audit (Include Runtime Tests)” – when checked, the audit will include dynamic analysis (Lighthouse performance audit, Playwright/Axe accessibility tests, PWA/offline checks, etc.). This will take longer and will actually build/run the project in a headless browser. By default (unchecked), the audit will be “static only” (analyzing code, configs, etc. without running the app). A note will explain the difference (e.g. “Full audit runs additional performance and end-to-end checks (may take a few minutes)”).
    o “Auto-Fix Issues Where Possible” – if enabled, the audit will attempt to automatically fix common issues (like applying codemod or lint fixes for trivial issues: inline styles, missing alt attributes, etc.). These fixes will be included as changes in the audit PR. (The toolkit supports --fix mode; if this is checked, we will add that flag to the audit command.)
    o “Create Pull Request (vs. Just a Branch)” – by default, a PR will be opened with results. If the user prefers not to open a PR (just push the audit branch so they can review it themselves), they can uncheck this option. In that case, the workflow will still push the branch with reports, but skip creating a PR. This might be useful for quick checks or for repos where they don't want an open PR. (Note: Even if no PR, the results would be in the branch in their repo.)
    o “Auto-Merge if Passing” – if checked, once the audit completes and passes all criteria (no critical issues, score above threshold), the audit PR will auto-merge into the default branch. This automates applying any fixes and adding reports to the codebase. This option will be clearly marked as potentially dangerous (only merges when no issues or only minor fixes). It will utilize the audit:auto-merge label or a direct merge action in the workflow.
    o These options make the front-end flexible for both team use (perhaps enabling more automation) and public use (who may just want a report, not code changes).
14. User Experience & Feedback: Emphasize a clean, simple interface. After submitting the repo and options, the user should see feedback:
    o A status indicator or progress log (e.g., "Queued, Running audit, Audit completed") that updates in real-time or via polling. We will implement a status endpoint (see Backend) and poll it to update the UI. Possibly show an animated loader or steps as outlined in the audit (environment check, scanning, etc., as available).
    o Once complete, display the key results: for example, overall grade (A/B/C/etc.), perhaps a summary like “✅ Passed” or “❌ Failed” based on whether minimum score/criteria met.
    o Provide a link to the GitHub PR that contains the full report and code changes. Also, a direct link to view the HTML report file (using GitHub’s raw file URL or via the Vercel backend proxying it). The UI might also show a snippet of the Markdown report (e.g. top 3 issues or summary stats) for convenience.
    o If errors occur (e.g. the workflow fails, or the repo is not accessible), display a clear error message with guidance (e.g. "Audit failed - please ensure the repository has the audit workflow and our app has access").
15. Responsive Design & Theming: Since this app itself uses Next.js and MUI, it should follow best practices (a bit meta, as it audits others). It will be mobile-friendly and themed appropriately. The design will be simple (it’s a utility app), focusing on clarity and not overwhelming the user with technical details. Possibly include a logo or branding for the tool.
16. Backend Services and API:
    Implement a lightweight backend (Node.js, integrated with the Next.js API routes or as a separate Express server if needed) to orchestrate the audit process. The backend’s responsibilities and deliverables:
17. Audit Orchestration Endpoints: Provide API endpoints as described in the design:
    o POST /create-audit – Accepts a JSON body with { repoUrl, mode, pat? }.
    o RepoUrl: e.g. "https://github.com/owner/repo" or "owner/repo". The backend will parse this into owner and repo and determine default branch if needed.
    o Mode: "app" or "pat", indicating which credential method to use.
    o PAT (optional): If mode is "pat", the personal access token provided by the user. On call, the backend will initiate the audit: verify credentials, then trigger the trigger workflow on our repo via GitHub API (this is effectively calling the repository_dispatch event). It will generate a unique runId (which could be an internal ID or we could use the GitHub Actions run ID or PR number as the reference) and respond immediately with { runId, prUrl: null } (prUrl might be unknown until later, unless we precompute a possible PR link).
    o GET /audit-status?runId=... – Returns the current status of the audit run. This will allow the front-end to poll for updates. The response JSON could be { state: "queued"|"running"|"completed"|"failed", grade: <string or number>, prUrl, artifacts: <links if any> }.
    o Implementation: The backend might track status by listening for GitHub webhooks (e.g. completion of the dispatch or check run) or by polling the GitHub Actions run and PR status. Simpler approach: poll GitHub for existence of the PR or check run. Alternatively, we can use the Checks API to get the conclusion of our "Audit Summary" check. The first version can poll every few seconds for a limited time until it finds a result.
    o When state is "completed", the response can include the final grade (if easily retrievable from the PR or reports) and the prUrl. If "failed", include an error message.
    o GET /auth/github/callback – Endpoint that GitHub will redirect to after the user installs the GitHub App. This will receive query params like installation_id. The backend will save the installation ID (and possibly the account ID who installed) in a session or database keyed to the user (if we have user sessions) or attach it to a cookie. This allows subsequent calls to know which installation/token to use. If no user login system is in place, we may handle this by storing a temporary token client-side. However, since the app is intended for any user, a minimal session mechanism will likely be needed (possibly storing an encrypted token in a cookie that references the installation).
    o After processing, this endpoint will redirect the user back to the front-end (e.g. to the form), possibly with a flash message like "App installed successfully. You can now run the audit."
18. Session Management: Since multiple users can use the platform, implement a minimal session or token system. Likely using JWT or Next.js built-in auth if needed, but it might be as simple as storing the installation ID in a cookie for that user. We will ensure this is secure and doesn't expose anything sensitive. (The installation ID alone is not a secret; the ability to create tokens is tied to our app’s private key which stays on server side).
19. Audit Workflow Invocation: The backend will contain the logic to call the GitHub API to dispatch the audit workflow. This involves using either:
    o The installation access token (for "app" mode): The backend will use the stored App credentials (APP ID, private key) to request an installation token via GitHub’s API, scoped to the target repo. Then call the repository dispatch event with that token.
    o The PAT (for "pat" mode): Use the PAT directly to call the repository dispatch on the repo (endpoint: POST /repos/{owner}/{repo}/dispatches).
    o Include the appropriate payload (selected options such as runtime: true/false, autoFix: true/false, etc.) so the target workflow knows what to do.
20. Real-time Feedback Mechanism: Possibly implement a WebSocket or Server-Sent Events channel to push updates to the front-end as an alternative to polling (if time permits). Given the relative short run time (likely under a couple of minutes for full audit), polling every few seconds via /audit-status may suffice for now. This can be revisited in future versions.
21. Error Handling & Retries: The backend will handle common failure cases gracefully, informing the user:
    o If the repository does not have the audit workflow file set up, detect the failure (the dispatch might still succeed but the workflow won’t run correctly). We will document that users need to add the run-audit.yml to their repo (or we can provide a one-click PR to add it, future enhancement). Possibly the backend can check the repo contents for that workflow file (GitHub API contents check) and warn if not present.
    o If the GitHub App is not installed or lacks permission on that repo, inform the user to install/authorize.
    o If the PAT is invalid or lacking scope, return an error immediately.
    o Use try/catch around API calls and provide useful error responses to the UI (which will display them).
22. Scalability: The backend’s role is fairly light (mostly API calls out to GitHub and keeping track of status), so it should scale easily on Vercel’s serverless platform. We will ensure any long polling or waiting is minimized (offloaded to front-end polling or short bursts).
23. Audit Toolkit Integration and Capabilities:
    The core of this platform is the Next.js + MUI Best-Practice Audit Toolkit v1.1.0, which will be fully integrated. We will ensure the platform leverages all its features, making the auditing as comprehensive and effective as possible. Deliverables related to the audit capabilities include:
24. Comprehensive Rule Coverage: The audit toolkit covers a wide range of best practices and potential issues in Next.js/MUI projects. The platform will utilize all these rules, ensuring the broadest coverage. The audit will evaluate:
    o Next.js Architecture (20% of score) – e.g. correct usage of App Router vs Pages, server vs client components, Image/Font optimization, proper metadata and head usage, SSR hydration consistency, etc.
    o MUI Usage (20%) – e.g. no inline CSS (prefer MUI sx props), consistent theme usage (no hard-coded colors or spacing outside theme tokens), proper responsive design with Material breakpoints, avoidance of deprecated MUI APIs, optimal MUI imports (no wildcard imports that bloat bundle), etc.
    o Accessibility (15%) – e.g. WCAG AA compliance checks: all images have alt text, form elements have labels, semantic HTML structure, ARIA attributes where needed, color contrast meets standards, no keyboard traps, etc.
    o Responsive Design (15%) – e.g. mobile-first CSS, correct use of viewport meta, no fixed dimensions that break on small screens, fluid typography, adequate tap target sizes.
    o Performance (10%) – e.g. bundle size analysis (each page’s JS under budget ~180KB), code-splitting and dynamic import usage, eliminating unused JS/CSS, checking for large dependencies (with warnings for known heavy libs), React optimization (no heavy computation on client, proper memoization).
    o Security (5%) – e.g. secret scanning (no API keys in code), basic web security (no obvious XSS vectors in dangerouslySetHTML etc., external links use rel="noopener"), checking for secure headers or config if available, ensuring usage of Next.js security features (like CSP in headers, or safe use of next/script).
    o Code Quality (10%) – e.g. ESLint rule compliance (the toolkit can run an ESLint suite), adherence to TypeScript strict rules, no unused variables/imports, reasonable file sizes and function complexity, proper React hook usage rules, etc.
    o Testing & Documentation (5%) – e.g. presence of unit/integration tests, coverage levels, existence of a thorough README and/or Storybook, etc.
    o Progressive Web App (PWA) & SEO (bonus) – the toolkit also checks PWA compliance (manifest, service worker, offline support) and SEO essentials (meta tags, structured data). These might factor into above categories or as extra credit.
    o Each category contributes to an overall letter grade. The deliverable here is that the system will compute and display this grade, and the detailed breakdown is available in the REPORT.md. Weights and thresholds (min score default 85 for pass) are configurable via the toolkit’s config.
25. Auto-Fix Implementation: The platform will support the toolkit’s auto-fix feature. If the user enabled “Auto-Fix” option, the audit will run in fix mode (nextjs-mui-audit run --fix). The toolkit will automatically correct certain issues (for example, replace inline styles with sx usage, add missing alt attributes, format code, etc.) and output a FIX_REPORT.md summarizing changes. The audit branch will include those fixes as part of the commit (or as separate commit). Deliverable: ensure that fixes are properly applied and documented, and that the PR highlights what was fixed automatically. (We will verify that auto-fixes do not break the app – they should be minor safe changes.)
26. Caching for Performance: Leverage the toolkit’s caching mechanism for faster subsequent audits. The first run on a repo will create a cache (e.g. in a temp directory or as an artifact). Subsequent runs use --cache to skip re-scanning unchanged files, potentially speeding up audits by 50-80%. In our GitHub Actions workflow, we can utilize actions/cache to persist the node_modules/.cache/nextjs-mui-audit or similar between runs on the same repo. This is an optimization deliverable to make the platform snappy, especially if a team runs audits frequently. (If using repository_dispatch, each run might be isolated; but we can attach to the same runner via concurrency: audit-${{ github.ref }} to potentially reuse cache on re-run.)
27. Interactive Reports: The HTML report generation is a major feature (with charts, filters, etc.). We will ensure this artifact is easily accessible. One deliverable is to possibly host the HTML report via GitHub Pages or via the Vercel app for convenience. For example, after audit, the platform could fetch the audit/report.html from the repo and display it in an embedded iframe or a new window. (Alternatively, instruct the user how to use the raw GitHub URL to view it.) We'll evaluate the best UX here (keeping security in mind if loading raw HTML). At minimum, we clearly link to it and note that it’s viewable by downloading and opening in a browser.
28. Plugin System & Custom Rules: The toolkit is extensible via plugins. Our SOW includes making the solution future-proof for custom checks. We will document how new rules/plugins can be added to the toolkit configuration (audit.config.js in a repo). While creating new plugins is outside this scope, we ensure that if a project has a custom config or plugin, our system will still run using it (e.g. if audit.config.js exists in the repo, the audit command will automatically load it). This allows power users to customize audits.
29. Latest Toolkit Version: We will use v1.1.0 of the toolkit, which includes all the latest features (auto-fix, interactive UI, improved logging, etc.). As part of deliverables, we will coordinate with the toolkit’s development (which may be the same team) to add a --ci or machine-readable summary flag. This will ensure our GitHub Action can easily get a JSON summary for use in check outputs. We’ll also lock a stable artifact file structure (audit/ folder as described) so our platform knows where to retrieve reports from.
30. Manual Checks Integration: The toolkit allows adding manual audit results (for things that require human validation). While primarily outside automation, we will ensure the generated Markdown report includes placeholders for manual checks (e.g. “responsive design tested on X devices – PASS/FAIL”). If the team chooses to, they can fill these in later and merge. (Not an automated part, but the SOW acknowledges this feature of the toolkit for completeness.)
31. Results Presentation and Reporting:
    Deliver a robust reporting mechanism so that users can easily understand and act on the audit findings. Key aspects:
32. Pull Request as Report Container: The primary mode of delivering results is the GitHub Pull Request in the user’s repository. This PR will contain:
    o The committed report files (as described: markdown, HTML, JSON, etc.). The Markdown report (audit/REPORT.md) provides a convenient in-situ summary with the most important findings and scores. The user can read this directly on the PR’s “Files changed” tab. The JSON can be used for any programmatic analysis.
    o An overview comment and/or PR description noting the overall grade and key highlights (we will template this text to be clear and concise).
    o If auto-fixes were applied, those will show up as code changes in the PR, alongside the reports. This way, the user can review what was changed in their code to fix issues. For example, “Removed inline style in pages/index.js and replaced with MUI sx prop” would appear as a diff, and the FIX_REPORT.md will list them too.
33. Status Check and PR Checks Tab: The posted status check (“Audit Summary”) will display a one-line summary with a pass/fail status. For example: ✔ Audit complete – Grade: B (87) or ❌ Audit complete – Grade: F (54). If failing, we mark the check as failed, which can prevent merging (if branch protection is set to require audit pass). This ensures teams cannot ignore a failing audit. The check’s “Details” link can point to the REPORT.md in the PR.
34. Web UI Summary: In addition to GitHub, our front-end will show the results summary as described. For completeness, deliver a results page or modal in the UI that, after an audit, shows:
    o The repository name and the time of audit completion.
    o Overall score/grade and perhaps a visual indicator (like a letter grade badge or percentage).
    o Number of critical issues found vs. fixed vs. remaining.
    o A direct link to open the GitHub PR to see details.
    o Possibly an embedded view of the top section of REPORT.md (like the grade and category breakdown). We can fetch the raw markdown or JSON from the branch via GitHub API and display a short summary (for instance, categories with their scores).
    o A message about next steps (e.g. “Review the Pull Request to see detailed findings and merge the recommended changes.”).
35. Multi-user Report Access: Since this is open to any user, ensure that the reports generated are accessible only to those with access to the repository (the PR is in their repo). Our platform will not copy or store the report contents on our servers beyond transiently for display. Privacy of their code and results is maintained (we only keep minimal logs). For open-source repos, this is not an issue; for private repos, the user must install the app which only grants our app access. We will not expose the content to any other party.
36. Historical Records (Future consideration): The scope focuses on on-demand audits, not continuous monitoring, but we will leave room for future expansion such as storing past audit results to show trends over time (e.g. grades improving). For now, each audit is a separate PR/branch in the repo, which itself serves as a historical record.
37. Security, Sandbox, and Performance Considerations:
    To make this platform truly robust and secure (critical for a GOAT-level tool), we will address various edge cases and enforce strict safety measures:
38. Sandboxed Execution: All audits run on GitHub’s infrastructure (Actions runners). No untrusted code runs on our server. This isolates any malicious code in the audited repo. The GH App’s permissions also prevent destructive actions (it can only write the audit folder and PR, it cannot, for example, deploy or push to protected branches). We use ephemeral tokens and do not hold elevated credentials.
39. Resource Limits: We will configure the GitHub Actions jobs with sensible limits. For example, if runtime audits are enabled, we ensure that the job doesn’t run indefinitely – Next.js build and Lighthouse tests can be time-consuming, so we might impose a timeout (e.g. 5-10 minutes per run). The workflow will have job.timeout-minutes set to prevent hanging. Memory usage is inherently limited by the GitHub runner (if a test uses too much memory, the runner will fail).
40. Handling Large Repos: For very large repositories, we optimize by:
    o Shallow cloning only the latest commit by default (unless the audit specifically needs history, which it does not).
    o Optionally using path filters: we can limit the checkout to only relevant paths (e.g. pages/**, src/**, app/**, components/**) to skip unrelated large directories. This will be done carefully to not miss any Next.js app code (monorepos might have the app in a subfolder, which we will detect or allow the user to specify a subdirectory in the UI if needed).
    o The audit toolkit itself might skip node_modules and other irrelevant folders by default; we ensure it doesn’t scan huge binary files, etc.
41. Monorepo and Multi-Project Support: If the target repository contains multiple projects (e.g. a monorepo with more than one Next.js app, or frontend/backend together), the audit should handle it gracefully:
    o We will allow an optional path input in the UI (or automatically detect) to specify the Next.js project root within the repo. This will be passed to the audit tool’s --path option to focus the audit.
    o Alternatively, the toolkit can try to auto-detect Next.js apps in a repo. If multiple are found, it could iterate through them. If not in scope for initial version, we document that only one app at a time is audited unless configured otherwise.
42. Private Submodules: In case the repo uses submodules (especially private ones), the checkout step can fail. We will address this by temporarily injecting our GitHub App token or PAT into the git auth (GIT_ASKPASS) so submodules can be fetched. This ensures the audit isn't blocked by missing code.
43. Branch Protection Compliance: Because we create a separate branch and PR, we respect any branch protection on the default branch. We do not push directly to main. The PR allows maintainers to review changes. If branch protection requires PR approvals, our auto-merge feature will only work if those checks (like our own audit check) pass and possibly if the repo allows bot auto-merge. We will not override any required reviews by default – that would be up to maintainers. This approach ensures we integrate smoothly into even the most locked-down repo setups.
44. Secret Management: The platform itself will securely handle secrets:
    o The GitHub App’s private key and ID will be stored as secure environment variables on Vercel (or in GitHub Actions for the trigger workflow). The backend uses them to authenticate to GitHub – never exposing them to the client.
    o If PAT is used, it is not stored on the server; it might only live in-memory for the request or an encrypted token in transit. We will log as little as possible (certainly not the PAT).
    o The audit workflow will be configured to mask any secrets it might encounter in logs. (By default, GitHub Actions masks things that match the form of secrets in the repo. Our own logs from the audit tool shouldn’t contain sensitive info, but if we print environment variables for debugging, we’ll be cautious.)
45. Data Privacy: We won’t store repository code or content on our platform; code stays on GitHub. The only things stored temporarily might be the installation ID mapping to a user session and the minimal audit status info. All results reside in the user’s repo. This makes the system inherently privacy-preserving for users.
46. Reliability & Retry: Implement measures for reliability, e.g. if triggering the audit fails due to GitHub API issues, the backend will retry a couple of times and inform the user to retry if needed. We also consider GitHub API rate limits – but since audits are user-initiated and relatively infrequent, this is unlikely an issue.
47. Performance: The added caching (as mentioned) and selective checks aim to keep audit runtime reasonable (ideally 1-3 minutes for typical projects, maybe up to 5-7 minutes for full Lighthouse tests). We will communicate to the user if a full audit might take a bit longer.
48. Testing Against Malicious Cases: As part of development, we will test the system on intentionally malformed or malicious repos (with known bad patterns) to ensure the sandbox holds and our tool doesn’t inadvertently run unsafe scripts. (For example, ensure that even if a project has a dangerous npm postinstall script, it’s not executed – since we use npm ci only for needed packages and can skip project’s own scripts.) The audit CLI will be invoked in a controlled way.
49. Edge Case: No Workflow on Repo: If a user installs the app but forgets to add the audit workflow file to their repo, our system can optionally offer to create a PR adding the workflow. This could be a nice enhancement: using our app’s commit permissions to add the .github/workflows/run-audit.yml to their repo. We can include this as a deliverable if time permits: a one-time setup PR. If not implemented initially, we will at least clearly document this requirement and check for it as noted.
50. Testing, QA, and Verification:
    To ensure the platform is reliable and truly the best, we will conduct thorough testing and include certain testing-related deliverables:
51. Unit and Integration Tests: Write automated tests for critical backend logic (e.g. parsing repo URL, handling the GitHub callbacks, and the status polling logic). Use a testing framework (like Jest for Node or integration tests possibly with supertest) to simulate API calls. Also test UI components (maybe basic rendering tests) and the state transitions on status updates.
52. End-to-End Testing of Workflow: Utilize a test GitHub repository (or multiple) to run full end-to-end audits. We will set up example Next.js projects (including some with intentional issues) to verify that:
    o The end-to-end flow works: user installs app, triggers audit, PR is created with correct content, etc.
    o The grade and output are as expected given the known issues in the test repo.
53. Toolkit Accuracy: Because the audit toolkit is complex, we will verify that integration is correct – e.g. if the toolkit returns a non-zero exit (fail due to low score), our workflow should mark the check as failure. We’ll test scenarios of pass and fail thresholds.
54. Cross-Browser and Responsiveness: Test the front-end on modern browsers and devices to ensure the UI is usable (especially given it might be used by developers on various platforms).
55. Performance Tests: Simulate multiple simultaneous audits to ensure the backend can handle them (the main load goes to GitHub Actions, which can run many in parallel, but our server should handle multiple incoming requests). Vercel should autoscale as needed.
56. Security Tests: If possible, conduct a basic security audit of the platform (e.g. no XSS in our UI, no leaking of tokens, etc.). Possibly invite a review since it's open source.
57. User Acceptance Testing: Gather feedback from the managing team (and possibly a few external beta users) to make sure the platform is intuitive and meets needs. We’ll refine the UI/wording as needed from this.
58. Documentation and Open-Source Deliverables:
    Comprehensive documentation will be provided for both end-users and developers, ensuring the project’s longevity as an open-source tool. Key documentation deliverables:
59. User Guide: A Markdown or website page explaining how to use the platform. This will include:
    o How to install the GitHub App and required workflow in your repo.
    o How to run an audit from the web UI (step-by-step with screenshots perhaps).
    o Explanation of each option (what “Full Audit” means, etc.).
    o How to interpret the results (understanding the grade, where to find the report, how to apply fixes, etc.).
    o Troubleshooting common issues (e.g. app not installed, no workflow file, etc.).
60. Developer Guide: For those who want to contribute or deploy their own instance:
    o Overview of the architecture (front-end, back-end, GitHub workflows).
    o Setup instructions for local development and for deployment (e.g. how to configure a GitHub App, and then set environment variables like APP_ID, APP_PRIVATE_KEY on Vercel).
    o Code repository structure documentation, explaining key files and how the pieces connect (for example, highlight where in the code the repository_dispatch is triggered, how the status is polled, etc.).
    o How to update the audit toolkit version in the future, or adjust rules. Also mention the plugin system for adding rules.
    o Testing instructions (how to run the test suite, how to perform an end-to-end test on a sample repo).
61. Open Source Repository: The code will be published in a GitHub repository (likely under the client’s account or organization). We will include a proper README.md summarizing the project, and a LICENSE (likely MIT or as chosen) to govern open-source use.
62. Vercel Deployment Config: Provide config files or guidelines for deploying to Vercel (e.g. vercel.json if needed, environment variable setup instructions). We will ensure the project can be easily deployed by others by committing any needed config (except secrets).
63. Changelog & Versioning: Although initially v1.0, we will maintain a CHANGELOG.md for future changes. The initial release notes will highlight the major features (possibly based on the toolkit’s features list).
64. Support and Contribution Guidelines: Include CONTACT info or issues link for support, and CONTRIBUTING.md to encourage community contributions (since it's open source).
    Project Timeline and Milestones (Tentative)
    (While exact dates are to be determined, the following outlines the sequence of implementation milestones in this SOW):
    • Week 1-2: Project Setup & GitHub App – Register GitHub App, configure permissions, implement basic OAuth flow and hello-world repository dispatch to ensure connectivity. Set up repository and deployment pipeline (Vercel).
    • Week 3-4: Backend & Workflow Development – Implement the backend endpoints (create-audit, etc.), create the trigger GitHub Action, and the receiver workflow file. Test dispatching to a sample repo with minimal stub actions.
    • Week 5-6: Integrate Audit Toolkit & Workflow Completion – Install and run the actual audit toolkit in the workflow. Fine-tune the workflow steps (reports generation, branch push, PR creation, status checks). Test with sample repos to generate real reports.
    • Week 7: Frontend UI Implementation – Build the UI form, options, and link it with the backend (calls to create-audit, polling status). Implement GitHub login/app install in UI. Ensure UI reflects statuses properly.
    • Week 8: Advanced Features & Edge Cases – Add support for optional features: full vs static audit toggle, auto-fix toggle, auto-merge, path selection for monorepos, PAT fallback handling in UI, etc. Implement caching in workflow, submodule handling, etc.
    • Week 9: Testing & Hardening – Conduct thorough testing (unit, integration, e2e as described). Address any security or performance issues found. Improve documentation.
    • Week 10: Deployment & Launch – Deploy the final app to production on Vercel. Do a final run-through with the team’s real repositories. Prepare open-source repository for public release (ensure all credentials and sensitive info are excluded, test that others can fork and run their own).
    • Post-launch: Collect feedback from initial users, fix any urgent bugs, and polish documentation.
    (The timeline can be adjusted based on priorities; for instance, if making it available to the team sooner is crucial, we can stage the release – an MVP with core auditing, then add full runtime tests and auto-fix in a second phase.)
    Conclusion
    In summary, this Statement of Work details the creation of a full-featured Next.js + MUI auditing platform tightly integrated with GitHub. The deliverables include everything needed to make this solution top-tier in code auditing: from a secure GitHub App and workflows, a user-friendly web interface with flexible options, to comprehensive audit rules and auto-fix capabilities provided by the audit toolkit. By the end of this project, the team and any developer in the community will be able to seamlessly audit their code with one click, get actionable insights and even automated fixes, all while keeping their workflow within GitHub. This will not only save developer time but also significantly improve code quality and maintainability across Next.js/MUI projects. The open-source nature ensures continuous improvement and trust in the tool. We are confident that implementing this SOW will indeed make the product the “GOAT” of code auditing solutions for modern web projects. The team is excited to bring this vision to reality.

---
